% Paper template for TAR 2022
% (C) 2014 Jan Šnajder, Goran Glavaš, Domagoj Alagić, Mladen Karan
% TakeLab, FER

\documentclass[10pt, a4paper]{article}

\usepackage{tar2023}

\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Unjust Equivalence: Are Irony and Sarcasm Truly the Same in NLP?}

\name{Bojan Puvača, Florijan Sandalj, Ivan Unković} 

\address{
University of Zagreb, Faculty of Electrical Engineering and Computing\\
Unska 3, 10000 Zagreb, Croatia\\ 
\texttt{\{bojan.puvaca, florijan.sandalj, ivan.unkovic\}@fer.hr}\\
}      
         
\abstract{ 
}

\begin{document}

\maketitleabstract

\section{Introduction}

Irony and sarcasm are two very commonly used concepts in everyday language, which makes the fact that the distinction 
between the two is so puzzling and controversial all the more interesting. In this paper, we take a closer look at the
currently available irony and sarcasm detection datasets and analyze the ways in which they distinguish between those two
concepts.

Our goal is to determine the practical usefulness of the currently most popular definition of irony and sarcasm in NLP, which
states that sarcasm is a sharp, mean-spirited form of irony. We conduct a meta-analysis of the performance of different
models on the tasks of irony and sarcasm detection in order to determine the amount of overlap between the two tasks and
whether or not this definition makes sense in practice by examining how well models trained on one task perform on the other.

As both irony and sarcasm are based on polarity, we hypothesize that models that perform well on one task will also perform
well on the other, in which case the current distinction between the two wouldn't be of much use, and we would be better off
either treating the detection of the two as the same task or creating a more useful distinction between the two in the context
of NLP.

In the discussion section, we aim to answer the following questions that could aid future research of irony
and sarcasm in NLP:

\begin{itemize}
   \item How much overlap is there between irony and sarcasm in the most popular datasets?
   \item Does separating irony and sarcasm detection make sense in practice?
   \item Are there models better suited for one task than the other?
   \item Should the terms "irony" and "sarcasm" be redefined in the context of NLP to make them more useful for research?
\end{itemize}

\section{Irony and sarcasm in NLP}
The relationship between irony and sarcasm is unfortunately a heavily contested subject in NLP. This problem is the easiest
to notice when looking at different sarcasm and irony datasets, where we can find cases when they are treated as completely
seperate concepts \citep{kaggle-tweets}, when sarcasm is treated as a subset of irony \citep{semeval-2018} or vice
versa \citep{iSarcasm}. Searching for a clear consensus in the realm of linguistics is a futile effort as well, however the
distinction explained in \ref{sec:sarcasm_definition} seems to be the most common one, both in NLP and in general.
\subsection{Related work}
After an overview of related work on this topic, two datasets were deemed useful for our research \citep{iSarcasm}, \citep{semeval-2018},
and they are better explained in the section on construction of datasets \ref{sec:dataset_construction}. From other attempts in classifying 
ironic speech, it's worth mentioning \citep{bertweet}, where the proposed BERTweet architecture was tested on the SemEval-2018 taskA which 
deals with irony classification. Also, in \citep{transformers4irony-2020}, the authors build on the RoBERTa \citep{roberta} model to tackle 
irony classification, which proved to be a success. In \citep{transformers4irony-2023}, a media associated with the tweets is also included 
in classification data. However, it was noticed that none of these works delved into the differences between irony and sarcasm and whether it 
is useful to treat them as separate concepts, which is the direction this paper takes.
 
 
\subsection{Sarcasm - irony's meaner cousin}\label{sec:sarcasm_definition}
The online Merriam-Webster dictionary defines sarcasm as "a sharp and often satirical or ironic utterance designed to 
cut or give pain" \citep{mw-dictionary}. The iSarcasm dataset \citep{iSarcasm} aligns with this definition, as 
the "sarcasm" label is a subset of the unfortunately named "sarcastic" label, which actually 
indicates any kind of ironic speech. 

In this context, irony refers to any type of speech that is based on polarity - whether that be pointing out the polarity 
between the expected and actual outcome of a situation (situational irony), or expressing with words the opposite of what 
we mean (verbal irony based on polarity). Although this definition works on paper, there are some pitfalls. Most notably, 
the line between sarcasm and verbal irony is unclear, as whether or not a statement is mean-spirited is subjective, while 
a clearly mean spirited statement can also be based on polarity. Also, tweets that contain irony and aren’t directed at a 
specific person can still be considered sarcastic, as they often target a group of people, concepts, ideas or themselves 
in the form of self-deprecating humor. How the object of the irony affects the classification is a question that remains unanswered.

All things considered, the iSarcasm \citep{iSarcasm} dataset
does a solid job at distinguishing between the two. However, the usefulness of this distinction is limited, as both concepts
are based on dishonest speech, meaning that in practice there isn't much use in distinguishing between the two.

In this paper, we will take a closer look at how different models perform on the seperate tasks of irony and sarcasm detection,
with the goal of determining the amount of overlap between the two tasks and the potential benefits of redefining their distinction.
We will do so using a combination of the iSarcasm \citep{iSarcasm} and the SemEval-2018 \citep{semeval-2018} 
datasets, both of which contain tweets that are labeled as either ironic or sarcastic based on this distinction.

\section{Experimental setup}

Three seperate datasets were created for the experiment of comparing irony and sarcasm detection, one containing tweets labeled
as ironic, one containing tweets labeled as sarcastic and the third one combining the first two.

All three datasets were constructed as a binary classification task, with neutral tweets, not containing any irony or sarasm,
being labeled as negative, and the tweets containing either irony or sarcasm being labeled as positive.

As the SemEval-2018 dataset contained significantly more
ironic tweets than sarcastic ones, we merged it with the iSarcasm dataset in order to produce larger and more balanced datasets.
We found this approach to be justified, as both datasets discerned between irony and sarcasm in a similar manner, both in
their explanations for the labels and upon manual inspection of the tweets. 

Various models were trained on all three of these tasks, after which their performance was evaluated on the test sets of all
three datasets in order to determine the amount of overlap between irony and sarcasm detection.

The same models were also trained and evaluated on the unmodified SemEval-2018 dataset, in order to showcase their performance
on a standard dataset for ironic speech detection.

In section \ref{sec:dataset_construction}, we will describe the process of constructing the datasets and in \ref{sec:models}
we will describe the models used in this experiment.

\subsection{Construction of the datasets}\label{sec:dataset_construction}

The positively labeled tweets for the sarcasm detection task were taken from both the iSarcasm dataset, using tweets labeled
as "sarcasm", which the authors define as "tweets that contradict the state of affairs and are critical towards an addressee"
\citep{iSarcasm} and the SemEval-2018 dataset, using tweets with the label "3", meaning "other irony" and defined as "instances
which show no polarity contrast between the literal and the intended evaluation, but are nevertheless ironic" 
\citep{semeval-2018}. Although this definition seems different than the one from the iSarcasm dataset, when manually
inspecting the contents of those tweets, we found that they almost exclusively contained the hashtag \#sarcasm and contained
sarcastic remarks. This makes sense given the fact that the tweets were manually annotated after being collected using the
\#sarcasm hashtag.

For the irony detection task we used the same method, only this time using the tweets labeled as "irony" from the iSarcasm
dataset, defined as "tweets that contradict the state of affairs but are not obviously critical towards an
addressee" \citep{iSarcasm} and the tweets with labels "1" and "2" from the SemEval-2018 dataset, meaning "verbal irony by means 
of a polarity contrast" and "situational irony" respectively \citep{semeval-2018}. All three of these categories were based
on a polarity contrast, which is why they were grouped together.

Negative examples for both tasks were taken from both the iSarcasm and the SemEval-2018 datasets, using tweets labeled as
not being sarcastic or ironic.

The reason for not including sarcastic tweets as negative examples in the irony detection task and vice versa is so we
can effectively use models trained on one task for some other task, as otherwise the models would be trained
to label the positive examples of the other task as negative.

The combined dataset was created by merging the sarcastic and ironic datasets, with the positive examples of both tasks
being labeled as positive, and the neutral examples being labeled as negative.

All three datasets were split into training, validation and test sets and undersampled in runtime to ensure that all three
tasks had the same number of positive and negative examples in all three sets in order to ensure their fair comparison. For
the combined dataset, we also considered its unbalanced variant with all the available data. This was done to see if the 
approach of seperating sarcastic and ironic data makes sense in the general case, or if there is enough overlap between 
the two concepts to treat irony and sarcasm as one task if we aren't concerned with the distinction between the two.

\subsection{Tweet preprocessing}
Because of the specific language used in tweets, as well as the presence of hashtags, links and mentions, we used the
tweet normalization method proposed by the authors of BERTweet \citep{bertweet} in order to preprocess the tweets. This method
performs subword tokenization and replaces mentions and links with special tokens. Although this method was proposed to be 
used with the BERTweet model, we found it's use to be beneficial for all models used in this experiment.

The hashtags used to find and scrape the tweets in the SemEval-2018 dataset (\#sarcasm, \#irony and \#not) were removed
from the tweets, as not to make the detection task trivial.

\subsection{Models}\label{sec:models}

As our goal was to compare the performance of state-of-the-art models on the tasks of irony and sarcasm detection, we 
focused on transformer-based models. Specifically we used the BERT \citep{devlin-etal-2019-bert} and BERTweet \citep{bertweet} models,
as they showed excellent performance on the SemEval-2018 task in previous research \citep{transformers4irony-2020,bertweet}.

BERTweet uses a modified pre-training procedure based on RoBERTa \citep{roberta} and is pretrained on a corpus of tweets, 
making it more suitable for the tweet classification tasks.

Both of these models were used with their out-of-the-box sequence classification configurations, fine-tuned on our 
different tasks. We will refer to these models as \textsc{BERT} and \textsc{BERTweet}.

We have also used BERT and BERTweet as encoders and fed their outputs to a bidirecitonal LSTM layer, followed by 
a dropout layer and a dense layer which performs the classification. We will refer to these models as \textsc{BERT+LSTM}
and \textsc{BERTweet+LSTM}. 

A convolution based approach was used in a similar manner, with two convolutional layers
replacing the LSTM layer. In these models, the convolutional layer was followed by a max pooling layer, a dropout layer
and a dense layer. We will refer to these models as \textsc{BERT+CNN} and \textsc{BERTweet+CNN}.

All of these models were trained on the three tasks described in section \ref{sec:dataset_construction} and the SemEval-2018
task, along with a simple baseline model based on an LSTM network and GloVe embeddings, which we will refer to as the
\textsc{Baseline} model. 

\subsection{Training setup}
All of the transformer-based models were optmimized using cross-entropy loss, the AdamW PyTorch optimizer and a
learning rate scheduler with warmup with hyperparameters optimized on a per-model basis. The baseline model
was optimized using only the Adam optimizer and cross-entropy loss.

Models were trained for up to 10 epochs, with batch size of 16 and early stopping based on the F1 score on the validation set.
For each task and model pair, 5 different runs were performed, with the results being averaged in the end.

\subsection{Evaluation}
All of the afformentioned models trained on the SemEval-2018 training set were evaluated on the appropriate 
SemEval-2018 test set, while the models trained on each of our three tasks were evaluated on all three of our test sets. 
The F1 metric was used as the primary metric for evaluating and comparing the different models.

\section{Results}

In this section, we will show the results on all of the models on the SemEval-2018 dataset (section \ref{semeval-results}).
After that, an analysis of the performance of the best models on our three tasks and the transferability of the models between
the tasks will be presented in section \ref{task-analysis}. Performance of all models on irony and sarcasm detection tasks
will be analyzed in section \ref{correlation} in order to determine the correlation between the model performances on the two tasks.

\subsection{SemEval-2018 dataset}\label{semeval-results}

The results of the models on the SemEval-2018 dataset can be seen in Table \ref{tab:semeval-2018}. The models based on
BERTweet outperformed the models based on BERT, which is in line with the results of previous research \citep{bertweet}.

Our models with additional LSTM and CNN layers came close to the performance of the \textsc{BERTweet} model and outperformed
the baseline and BERT-based models. Based on these results, the models we will consider for the transferabilty tests are the
\textsc{BERTweet} and the \textsc{BERTweet+LSTM} models.

\begin{table}[h]
   \caption{Results of the models on the SemEval-2018 dataset}
   \label{tab:semeval-2018}
   \begin{center}
   \begin{tabular}{|c|c|c|}
   \toprule
   Irony classification model & F1 & Acc\\
   \midrule
   \textsc{Baseline} & 0.625 & 0.645 \\
   \textsc{BERT} & 0.654 & 0.654 \\
   \textsc{BERTweet} & \textbf{0.785} & \textbf{0.788} \\
   \textsc{BERT+LSTM} & 0.657 & 0.657 \\
   \textsc{BERTweet+LSTM} & 0.763 & 0.768 \\
   \textsc{BERT+CNN} & 0.669 & 0.670 \\
   \textsc{BERTweet+CNN} & 0.752 & 0.754 \\
   \bottomrule
   \end{tabular}
   \end{center}
\end{table}

\subsection{Irony and sarcasm transferabilty analysis}\label{task-analysis}

Tables \ref{tab:bertweet-results} and \ref{tab:bertweet-bilstm-results} show the results of the \textsc{BERTweet} and
\textsc{BERTweet+LSTM} trained on the three tasks, as well as the unbalanced version of the combined task. For each model
and training setup, we showcas

\begin{table}[h!]
   \centering
   \begin{tabular}{|c|c|c|c|}
       \hline
        & \textbf{Irony} & \textbf{Sarcasm} & \textbf{Mixed} \\ \hline
       \textbf{Irony} & 0.720 & 0.634 & 0.657 \\ \hline
       \textbf{Sarcasm} & 0.622 & 0.710 & 0.682 \\ \hline
       \textbf{Mixed} & 0.681 & 0.767 & 0.723 \\ \hline
       \textbf{Mixed unbalanced} & 0.659 & 0.772 & 0.726\\ \hline
   \end{tabular}
   \caption{F1 score matrix for model \textsc{BERTweet} on the three tasks. Rows represent the task the model was trained on, 
   while columns represent the task the model was evaluated on}
   \label{tab:bertweet-results}
\end{table}

\begin{table}[h!]
   \centering
   \begin{tabular}{|c|c|c|c|}
       \hline
       & \textbf{Irony} & \textbf{Sarcasm} & \textbf{Mixed} \\ \hline
       \textbf{Irony} & 0.759 & 0.682 & 0.720 \\ \hline
       \textbf{Sarcasm} & 0.547 & 0.743 & 0.660 \\ \hline
       \textbf{Mixed} & 0.698 & 0.775 & 0.736 \\ \hline
       \textbf{Mixed unbalanced} & 0.666 & 0.745 & 0.719\\ \hline
   \end{tabular}
   \caption{F1 score matrix for model \textsc{BERTweet+LSTM} on the three tasks}
   \label{tab:bertweet-bilstm-results}
\end{table}

\subsection{Irony and sarcasm performance analysis}\label{correlation}

The Table \ref{tab:correlation} shows the F1 scores of all the models on the tasks of irony and sarcasm detection.
For this analysis the models were simply trained and evaluated on the tasks of irony and sarcasm detection. 
This data will be used to determine the correlation between model performance on the two tasks.

\begin{table}[h!]
   \caption{Irony and sarcasm detection F1 scores}
   \label{tab:correlation}
   \begin{center}
   \begin{tabular}{|c|c|c|}
   \toprule
   Model & Irony F1 & Sarcasm F1 \\
   \midrule
   \textsc{Baseline} & 0.631 & 0.548 \\
   \textsc{BERT} & 0.713 & 0.703 \\
   \textsc{BERTweet} & 0.720 & 0.710 \\
   \textsc{BERT+LSTM} & 0.704 & 0.668 \\
   \textsc{BERTweet+LSTM} & 0.759 & 0.743 \\
   \textsc{BERT+CNN} & 0.697 & 0.706 \\
   \textsc{BERTweet+CNN} & 0.767 & 0.746 \\
   \bottomrule
   \end{tabular}
   \end{center}
\end{table}

\section{Discussion}\label{discussion}
For the discussion part of this paper, we will answer the questions posed in the introduction, based on the results.

\subsection{Overlap between irony and sarcasm detection}
The results shown in tables \ref{tab:bertweet-results} and \ref{tab:bertweet-bilstm-results} show solid transferability between 
the tasks of irony and sarcasm detection, with the BERTweet based models trained on one task performing decently well on the other.

The dropoff in performance when training on one task and evaluating on the other goes both ways, with the dropoff in both cases 
being of a similar magnitude. This suggests that framing sarcasm as a subset of irony in NLP doesn't tell the whole story, as
in that case we would expect larger dropoffs in performance when training on the sarcasm task and evaluating on the irony task than
the other way around.

This can be explained by the fact that while clearly not all ironic statements are sarcastic, a lot of the statements we perceive 
as sarcastic due to their tone don't actually contain any polarity and are therefore not picked up by the models trained on the
irony detection tasks as ironic. In that case, irony and detection would have more of an overlapping relationship than a hierarchical
one.

\subsection{Usefulness of irony and sarcasm data separation}
The results of the models trained on the combined dataset show excellent performance on the combined task, with the unbalanced 
variant with more data not showing much of a performance increase. Surprisingly, the models trained on the combined dataset dominated
the task of sarcasm detection, even more so than the models trained on the sarcasm detection task, while irony detection proved to be more
challenging.

Nevertheless, the results of the models trained on the combined dataset show that if our goal is to simply detect dishonest speech in
general, we shouldn't be bothered with seperating irony and sarcasm in our datasets, as this approach doesn't seem to be beneficial
when compared with combining the two tasks.

\subsection{Suitability of different models for irony and sarcasm detection}
The F1 scores obtained for models trained and tested on both irony and sarcasm indicate minimal differences between the tasks.
To further test the similarity between the tasks, we calculated the correlation between the F1 scores of our models using the pearson coeffiecient.
$$
r = 0.9421
$$
The coeffiecient indicates that models performing well in irony detection also demonstrate strong performance in sarcasm detection and vice versa. 
Consequently, there appears to be no justification for the utilization of separate models for detecting irony and sarcasm, as the tasks exhibit strong
similarity.
\subsection{Redefinition of "irony" and "sarcasm" in NLP}
Developing more distinct definitions of irony and sarcasm could be more beneficial in future research in NLP due to the shown 
similarities in their respective classification tasks. For example, the term "irony" could be 
defined exclusively as situational irony, whereas the term "sarcasm" could refer to all kinds of speech characterized by polarity.
Alternatively, even less emphasis could be placed on their distinction, treating irony and sarcasm detection
as closely related sequence classification tasks.
\section{Conclusion}

\bibliographystyle{tar2023}
\bibliography{tar2023} 

\end{document}

\section*{Acknowledgements}

\bibliographystyle{tar2023}
\bibliography{tar2023} 


