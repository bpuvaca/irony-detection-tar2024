% Paper template for TAR 2022
% (C) 2014 Jan Šnajder, Goran Glavaš, Domagoj Alagić, Mladen Karan
% TakeLab, FER

\documentclass[10pt, a4paper]{article}

\usepackage{tar2023}

\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Unjust Equivalence: Are Irony and Sarcasm Truly the Same in NLP?}

\name{Bojan Puvača, Florijan Sandalj, Ivan Unković} 

\address{
University of Zagreb, Faculty of Electrical Engineering and Computing\\
Unska 3, 10000 Zagreb, Croatia\\ 
\texttt{\{bojan.puvaca, florijan.sandalj, ivan.unkovic\}@fer.hr}\\
}      
         
\abstract{ 
   % Forms of speech like irony and sarcasm, where the actual sentiment and the words being spoken differ or even 
   % oppose, sometimes pose a problem even for humans. Furthermore, detecting such forms of speech in plain text
   % like tweets or messages is an even harder task due to the lack other social cues such as tone, intonation, pacing,
   % pauses etc. Therefore, it is of great importance to define the task at hand precisely, and make sure that the
   % distinctions are clear and that they contribute to the solving process. 
   Forms of dishonest speech like irony and sarcasm present significant challenges in natural language processing (NLP) 
   due to their inherently complex and context-dependent nature. Moreover, the thin line between irony and sarcasm is 
   often defined with inconsistency, which brings into question its usefulness.
   In this paper, we take a look at how the difference between irony and sarcasm is defined in the field of NLP,
   how different models perform on different tasks of their detection, how and where those tasks overlap and finally,
   try to conclude does solving those tasks separately even make practical sense.  }
%    The relationship between irony and sarcasm is unfortunately a heavily contested subject in NLP. This problem is the easiest
% to notice when looking at different sarcasm and irony datasets, where we can find cases when they are treated as completely
% seperate concepts \citep{kaggle-tweets}, when sarcasm is treated as a subset of irony \citep{semeval-2018} or vice
% versa \citep{iSarcasm}. Searching for a clear consensus in the realm of linguistics is a futile effort as well, however the
% distinction explained in \ref{sec:sarcasm_definition} seems to be the most common one, both in NLP and in general.



\begin{document}

\maketitleabstract

   


\section{Introduction}

Irony and sarcasm being the main representatives of dishonest speech makes their detection a mainstay problem in NLP. However,
their relationship often confuses researchers, as the way they are defined in various datasets and research papers is far from
consistent. Analyzing the results of our research, we hope to shed some light on this issue by answering some key questions regarding
the way problems based on irony and sarcasm should be tackled in NLP.

Our goal is to determine the practical usefulness of the currently most popular definition of irony and sarcasm in NLP, which
states that sarcasm is a sharp, mean-spirited form of irony. We conduct a meta-analysis of the performance of different
models on the tasks of irony and sarcasm detection in order to determine the amount of overlap between the two tasks and
whether or not this definition makes sense in practice by examining how well models trained on one task perform on the other.
Our \emph{code}\footnote{\scriptsize\texttt{https://github.com/bpuvaca/irony-detection-tar2024}} is publicly available.

As both irony and sarcasm 
% are based on polarity
can be forms of dishonest speech, 
we hypothesize that models that perform well on one task will also perform
well on the other, in which case the current distinction between the two wouldn't be of much use, and we would be better off
either treating the detection of the two as the same task or creating a more useful distinction between the two in the context
of NLP.

In the discussion section, we aim to answer the following questions that could aid future research of irony
and sarcasm in NLP:

\begin{itemize}
   \item How much overlap is there between irony and sarcasm in the most popular datasets?
   \item Does separating irony and sarcasm detection make sense in practice?
   \item Are there models better suited for one task than the other?
   \item Should the terms "irony" and "sarcasm" be redefined in the context of NLP to make them more useful for research?
\end{itemize}

\section{Irony and sarcasm in NLP}
This chapter takes a look at the related relevant works in NLP that deal with irony and sarcasm, and introduces our own view 
of the problem.
\subsection{Related work}
The datasets we found to be the most popular and relevant in irony and sarcasm detection research are iSarcasm \citep{iSarcasm} 
and SemEval \citep{semeval-2018}. In iSarcasm, the collected tweets were classed into five categories of ironic speech, two of
which were useful for this research:
\begin{enumerate}
   \item \textit{Sarcasm}: tweets that contradict the state of affairs and are critical towards an addressee
   \item \textit{Irony}:  tweets that contradict the state of affairs but are not obviously critical towards an addressee
 \end{enumerate}
In SemEval-2018,
two datasets are presented: taskA, which has any form of ironic speech labeled as 1, and non regular speech labeled 
with 0, and taskB, where the ironic speech is further classed into three categories:
\begin{enumerate}
   \item \textit{Verbal irony by means of a polarity contrast}
   instances containing an
   evaluative expression whose polarity (positive,
   negative) is inverted between the literal and the intended evaluation
   \item \textit{Situational irony}:  instances describing situational irony, or situations
   that fail to meet some expectations
   \item \textit{Other verbal irony}: instances that show no polarity contrast between the 
   literal and the intended evaluation, but are nevertheless ironic
 \end{enumerate}
 In both datasets the tweets were manually annotated after being collected using specific hashtags, such 
 as \#irony, \#not and \#sarcasm.

From actual attempts at classifying 
ironic speech, it's worth mentioning \citep{bertweet}, where the proposed BERTweet architecture uses a modified pre-training procedure based 
on RoBERTa \citep{roberta} and is pretrained on a corpus of tweets, 
making it more suitable for tweet classification tasks. It was tested on the SemEval-2018 taskA which 
deals with binary classification of irony and produced competitive results. Also, in \citep{transformers4irony-2020}, the authors build on the 
RoBERTa model to tackle 
irony classification, which proved to be a success. In \citep{transformers4irony-2023}, images associated with the tweets are also used 
for classification. However, none of these works explored the differences between irony and sarcasm and whether it 
is useful to treat them as separate concepts, which is the direction this paper takes.
 
 
\subsection{Sarcasm - irony's meaner cousin}\label{sec:sarcasm_definition}
The online Merriam-Webster dictionary defines sarcasm as "a sharp and often satirical or ironic utterance designed to 
cut or give pain" \citep{mw-dictionary}. The iSarcasm dataset \citep{iSarcasm} aligns with this definition, as 
the "sarcasm" label is a subset of the unfortunately named "sarcastic" label, which actually 
indicates any kind of ironic speech. 

In this context, irony refers to any type of speech that is based on polarity - whether that be pointing out the polarity 
between the expected and actual outcome of a situation (situational irony, in which case there is no dishonest speech), or expressing with words the opposite of what 
we mean (verbal irony based on polarity). On the other hand, sarcasm refers to instances that show no polarity contrast between the
literal and the intended meaning, but are nevertheless ironic \citep{semeval-2018}.
Although this definition works on paper, there are some pitfalls. Most notably, 
the line between sarcasm and verbal irony is unclear, as whether or not a statement is mean-spirited is subjective.
Statements with a mean tone, but without polarity can also be considered sarcastic, meaning that sarcasm doesn't necessarily entail irony.
Also, tweets that contain irony and aren’t directed at a 
specific person can still be considered sarcastic, as they often target a group of people, concepts, ideas or themselves 
in the form of self-deprecating humor. How the object of the irony affects the classification is another unanswered question.

All things considered, the iSarcasm \citep{iSarcasm} dataset
does a solid job at distinguishing between the two. However, the usefulness of this distinction is somewhat questionable, 
as both concepts can be based on dishonest speech, meaning that in practice there might not be much use in distinguishing
 between them.

In this paper, we will take a closer look at how different models perform on the seperate tasks of irony and sarcasm detection,
with the goal of determining the amount of overlap between the two tasks and the potential benefits of redefining their distinction.
We will do so using a combination of the iSarcasm \citep{iSarcasm} and the SemEval-2018 \citep{semeval-2018} 
datasets, both of which contain tweets that are labeled as either ironic or sarcastic based on this distinction.

\section{Experimental setup}

Three seperate datasets were created for the experiment of comparing irony and sarcasm detection, one containing tweets labeled
as ironic, one containing tweets labeled as sarcastic and the third one combining the first two.

All three datasets were constructed as a binary classification task, with neutral tweets, not containing any irony or sarasm,
being labeled as negative, and the tweets containing either irony or sarcasm being labeled as positive.

As the SemEval-2018 dataset contained significantly more
ironic tweets than sarcastic ones, we merged it with the iSarcasm dataset in order to produce larger and more balanced datasets.
We found this approach to be justified, as both datasets discerned between irony and sarcasm in a similar manner, both in
their explanations for the labels and upon manual inspection of the tweets. 

Various models were trained on all three of these tasks, after which their performance was evaluated on the test sets of all
three datasets in order to determine the amount of overlap between irony and sarcasm detection.

The same models were also trained and evaluated on the unmodified SemEval-2018 dataset, in order to showcase their performance
on a standard dataset for ironic speech detection.

In section \ref{sec:dataset_construction}, we will describe the process of constructing the datasets and in \ref{sec:models}
we will describe the models used in this experiment.

\subsection{Construction of the datasets}\label{sec:dataset_construction}

The positively labeled tweets for the sarcasm detection task were taken from both the iSarcasm and SemEval-2018 
datasets, using tweets labeled as "sarcasm" and "other verbal irony", respectively. Although their definitions 
seem different, when manually inspecting the contents of those tweets, we found that they almost exclusively 
contained the hashtag \#sarcasm and contained sarcastic remarks. 

For the irony detection task we used the same method, only this time using the tweets labeled as "irony" from the iSarcasm
dataset, and the tweets from the first and second category from the SemEval-2018 dataset, meaning "verbal irony by means 
of a polarity contrast" and "situational irony" respectively. All three of these categories were based
on a polarity contrast, which is why they were grouped together.

Negative examples for both tasks were taken from both the iSarcasm and the SemEval-2018 datasets, using tweets labeled as
not being sarcastic or ironic.

The reason for not including sarcastic tweets as negative examples in the irony detection task and vice versa is so we
can effectively use models trained on one task for some other task, as otherwise the models would be trained
to label the positive examples of the other task as negative.

The combined dataset was created by merging the sarcastic and ironic datasets, with the positive examples of both tasks
being labeled as positive, and the neutral examples being labeled as negative.

All three datasets were split into training, validation and test sets using a 60/20/20 split and undersampled in runtime to ensure that all three
tasks had the same number of positive and negative examples in all three sets in order to ensure their fair comparison. For
the combined dataset, we also considered its unbalanced variant with all the available data. This was done to see if the 
approach of seperating sarcastic and ironic data makes sense in the general case, or if there is enough overlap between 
the two concepts to treat irony and sarcasm as one task if we aren't concerned with the distinction between the two.

\subsection{Tweet preprocessing}
Because of the specific language used in tweets, as well as the presence of hashtags, links and mentions, we used the
tweet normalization method proposed by the authors of BERTweet \citep{bertweet} in order to preprocess the tweets. This method
performs subword tokenization and replaces mentions and links with special tokens. Although this method was proposed to be 
used with the BERTweet model, we found it's use to be beneficial for all models used in this experiment.

The hashtags used to find and scrape the tweets in the SemEval-2018 dataset (\#sarcasm, \#irony and \#not) were removed
from the tweets, as not to make the detection task trivial.

\subsection{Models}\label{sec:models}

As our goal was to compare the performance of state-of-the-art models on the tasks of irony and sarcasm detection, we 
focused on transformer-based models. Specifically we used the BERT \citep{devlin-etal-2019-bert} and BERTweet \citep{bertweet} models,
as they showed excellent performance on the SemEval-2018 task in previous research \citep{transformers4irony-2020,bertweet}.

Both of these models were used with their out-of-the-box sequence classification configurations, fine-tuned on our 
different tasks \citep{wolf2020huggingfaces}. We will refer to these models as \textsc{BERT} and \textsc{BERTweet}.

We have also used BERT and BERTweet as encoders and fed their outputs to a bidirecitonal LSTM layer from the PyTorch library
\citep{paszke2017automatic}, followed by a dropout layer and a dense layer which performs the classification. We will refer
to these models as \textsc{BERT+LSTM} and \textsc{BERTweet+LSTM}. 

A convolution based approach was used in a similar manner, with two convolutional layers
replacing the LSTM layer. In these models, the convolutional layer was followed by a max pooling layer, a dropout layer
and a dense layer. We will refer to these models as \textsc{BERT+CNN} and \textsc{BERTweet+CNN}.

All of these models were trained on the three tasks described in section \ref{sec:dataset_construction} and the SemEval-2018
task, along with a simple baseline model based on an LSTM network and GloVe embeddings, which we will refer to as the
\textsc{Baseline} model. 

\subsection{Training setup}
% All of the transformer-based models were optmimized using cross-entropy loss, the AdamW PyTorch optimizer and a
% learning rate scheduler with warmup with hyperparameters optimized on a per-model basis. The baseline model
% was optimized using only the Adam optimizer and cross-entropy loss.

All of the transformer-based models were trained using cross-entropy loss, the AdamW PyTorch optimizer and a
learning rate scheduler with a warmup. The hyperparameters were optimized on a per-model basis using a grid 
search over the combined and balanced dataset. The baseline model
was trained using only the Adam optimizer and cross-entropy loss.

Models were trained for up to 10 epochs, with batch size of 16 and early stopping based on the F1 score on the validation set.
For each task and model pair, 5 different runs were performed, with the results being averaged in the end.

\subsection{Evaluation}
All of the afformentioned models trained on the SemEval-2018 training set were evaluated on the appropriate 
SemEval-2018 test set, while the models trained on each of our three tasks were evaluated on all three of our test sets. 
The F1 metric was used as the primary metric for evaluating and comparing the different models.

\section{Results}

In this section, we will show the results on all of the models on the SemEval-2018 dataset (section \ref{semeval-results}).
After that, an analysis of the performance of the best models on our three tasks and the transferability of the models between
the tasks will be presented in section \ref{task-analysis}. Performance of all models on irony and sarcasm detection tasks
will be analyzed in section \ref{correlation} in order to determine the correlation between the model performances on the two tasks.

\subsection{SemEval-2018 dataset}\label{semeval-results}

The results of the models on the SemEval-2018 dataset can be seen in Table \ref{tab:semeval-2018}. The models based on
BERTweet outperformed the models based on BERT, which is in line with the results of previous research \citep{bertweet}.

Our models with additional LSTM and CNN layers came close to the performance of the \textsc{BERTweet} model and outperformed
the baseline and BERT-based models. Based on these results, the models we will consider for the transferabilty tests are the
\textsc{BERTweet} and the \textsc{BERTweet+LSTM} models.

\begin{table}[h]
   \caption{Results of the models on the SemEval-2018 dataset}
   \label{tab:semeval-2018}
   \begin{center}
   \begin{tabular}{|c|c|c|}
   \toprule
   Irony classification model & F1 & Acc\\
   \midrule
   \textsc{Baseline} & 0.625 & 0.645 \\
   \textsc{BERT} & 0.654 & 0.654 \\
   \textsc{BERTweet} & \textbf{0.785} & \textbf{0.788} \\
   \textsc{BERT+LSTM} & 0.657 & 0.657 \\
   \textsc{BERTweet+LSTM} & 0.763 & 0.768 \\
   \textsc{BERT+CNN} & 0.669 & 0.670 \\
   \textsc{BERTweet+CNN} & 0.752 & 0.754 \\
   \bottomrule
   \end{tabular}
   \end{center}
\end{table}

\subsection{Irony and sarcasm transferabilty results}\label{task-analysis}

Tables \ref{tab:bertweet-results} and \ref{tab:bertweet-bilstm-results} show the results of the \textsc{BERTweet} and
\textsc{BERTweet+LSTM} trained on the three tasks, as well as the unbalanced version of the combined task. For each model
and training setup, we showcase the F1 score on each of the three tasks.

\begin{table}[h!]
   \centering
   \begin{tabular}{|c|c|c|c|}
       \hline
        & \textbf{Irony} & \textbf{Sarcasm} & \textbf{Mixed} \\ \hline
       \textbf{Irony} & 0.720 & 0.634 & 0.657 \\ \hline
       \textbf{Sarcasm} & 0.622 & 0.710 & 0.682 \\ \hline
       \textbf{Mixed} & 0.681 & 0.767 & 0.723 \\ \hline
       \textbf{Mixed unbalanced} & 0.659 & 0.772 & 0.726\\ \hline
   \end{tabular}
   \caption{F1 score matrix for model \textsc{BERTweet} on the three tasks. Rows represent the task the model was trained on, 
   while columns represent the task the model was evaluated on}
   \label{tab:bertweet-results}
\end{table}

\begin{table}[h!]
   \centering
   \begin{tabular}{|c|c|c|c|}
       \hline
       & \textbf{Irony} & \textbf{Sarcasm} & \textbf{Mixed} \\ \hline
       \textbf{Irony} & 0.759 & 0.682 & 0.720 \\ \hline
       \textbf{Sarcasm} & 0.547 & 0.743 & 0.660 \\ \hline
       \textbf{Mixed} & 0.698 & 0.775 & 0.736 \\ \hline
       \textbf{Mixed unbalanced} & 0.666 & 0.745 & 0.719\\ \hline
   \end{tabular}
   \caption{F1 score matrix for model \textsc{BERTweet+LSTM} on the three tasks}
   \label{tab:bertweet-bilstm-results}
\end{table}

\subsection{Irony and sarcasm performance analysis}\label{correlation}

The Table \ref{tab:correlation} shows the F1 scores of all the models on the tasks of irony and sarcasm detection.
For this analysis the models were simply trained and evaluated on the tasks of irony and sarcasm detection. 
This data will be used to determine the correlation between model performance on the two tasks.

\begin{table}[h!]
   \caption{Irony and sarcasm detection F1 scores}
   \label{tab:correlation}
   \begin{center}
   \begin{tabular}{|c|c|c|}
   \toprule
   Model & Irony F1 & Sarcasm F1 \\
   \midrule
   \textsc{Baseline} & 0.631 & 0.548 \\
   \textsc{BERT} & 0.713 & 0.703 \\
   \textsc{BERTweet} & 0.720 & 0.710 \\
   \textsc{BERT+LSTM} & 0.704 & 0.668 \\
   \textsc{BERTweet+LSTM} & 0.759 & 0.743 \\
   \textsc{BERT+CNN} & 0.697 & 0.706 \\
   \textsc{BERTweet+CNN} & 0.767 & 0.746 \\
   \bottomrule
   \end{tabular}
   \end{center}
\end{table}

\section{Discussion}\label{discussion}
For the discussion part of this paper, we will answer the questions posed in the introduction, based on the results.

\subsection{Overlap between irony and sarcasm detection}
The results shown in tables \ref{tab:bertweet-results} and \ref{tab:bertweet-bilstm-results} show solid transferability between 
the tasks of irony and sarcasm detection, with the BERTweet based models trained on one task performing decently well on the other.

The dropoff in performance when training on one task and evaluating on the other goes both ways, with the dropoff in both cases 
being of a similar magnitude. This suggests that framing sarcasm as a subset of irony in NLP doesn't tell the whole story, as
in that case we would expect larger dropoffs in performance when training on the sarcasm task and evaluating on the irony task than
the other way around.

This can be explained by the fact that while clearly not all ironic statements are sarcastic, a lot of the statements we perceive 
as sarcastic due to their tone don't actually contain any polarity and are therefore not picked up by the models trained on the
irony detection tasks as ironic. In that case, irony and detection would have more of an overlapping relationship than a hierarchical
one.

\subsection{Usefulness of irony and sarcasm data separation}
The results of the models trained on the combined dataset show excellent performance on the combined task, with the unbalanced 
variant with more data not showing much of a performance increase. Surprisingly, the models trained on the combined dataset dominated
the task of sarcasm detection, even more so than the models trained on the sarcasm detection task, while irony detection proved to be more
challenging.

Nevertheless, the results of the models trained on the combined dataset show that if our goal is to simply detect dishonest speech in
general, we shouldn't be bothered with seperating irony and sarcasm in our datasets, as this approach doesn't seem to be beneficial
when compared with combining the two tasks.

\subsection{Suitability of different models for irony and sarcasm detection}
The F1 scores obtained for models trained and tested on both irony and sarcasm indicate minimal differences between the tasks.
To further test the similarity between the tasks, we calculated the correlation between the F1 scores of our models using the pearson coeffiecient.
$$
r = 0.9421
$$
The coeffiecient indicates that models performing well in irony detection also demonstrate strong performance in sarcasm detection and vice versa. 
Consequently, there appears to be no justification for the utilization of separate models for detecting irony and sarcasm, as the tasks exhibit strong
similarity.
\subsection{Redefinition of "irony" and "sarcasm" in NLP}
Developing more distinct definitions of irony and sarcasm could be more beneficial in future research in NLP due to the shown 
similarities in their respective classification tasks. For example, the term "irony" could be 
defined exclusively as situational irony, whereas the term "sarcasm" could refer to all kinds of speech characterized by polarity.
Alternatively, even less emphasis could be placed on their distinction, treating irony and sarcasm detection
as closely related sequence classification tasks.

We would like to see these suggestions considered in future datasets, as they could make the landscape of irony and sarcasm detection more consistent.
\section{Conclusion}
In order to disambiguate the relationship between irony and sarcasm in NLP, we conducted a meta-analysis of the performance of different models on the 
seperate tasks of irony and sarcasm detection. The results of our analysis provide useful insights into how these two concepts should be treated in the
future research.

Specifically, seperating the two tasks doesn't seem to be beneficial, as state-of-the-art models that perform well on one task, also perform
well on the other. The overlap between the two concepts also seems to be large enough such that labeling them differently in datasets and/or 
detecting them separately isn't of much use.

Consequently, we propose a redefinition of the terms "irony" and "sarcasm" in NLP, either by further distinguishing between the two or by treating
their detection as an even more related task.
\bibliographystyle{tar2023}
\bibliography{tar2023} 

\end{document}

\section*{Acknowledgements}

\bibliographystyle{tar2023}
\bibliography{tar2023} 


