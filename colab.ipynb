{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "JY2-kNQPGbxp"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/bpuvaca/irony-detection-tar2024.git\n",
        "%cd irony-detection-tar2024/nemojte/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "id": "C7MqE8irP99u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "# Must install separately since Colab has torch 2.2.1, which breaks packages\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "pass"
      ],
      "metadata": {
        "id": "c4glKyMuHSKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(model_name = \"unsloth/llama-3-8b-bnb-4bit\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "mna_news_input = \"\"\"HCL Technologies has completed acquisition of 51% stake in German IT Consulting Company, Gesellschaft für Banksysteme GmbH (“GBS”). The acquisition was completed on January 05, 2022.\"\"\"\n",
        "mna_news_instruction = \"\"\"Which company is the Acquirer. \"\"\"\n",
        "\n",
        "alpaca_prompt.format(mna_news_instruction, mna_news_input, \"\")\n",
        "\n",
        "inputs = tokenizer([alpaca_prompt.format(mna_news_instruction, mna_news_input, \"\")], return_tensors = \"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens = 90)\n",
        "tokenizer.batch_decode(outputs)[0]"
      ],
      "metadata": {
        "id": "KNzPOT-_oNce"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}