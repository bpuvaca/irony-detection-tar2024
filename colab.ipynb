{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "JY2-kNQPGbxp",
        "outputId": "0c745e54-2a35-4f4f-d8cd-221f989e7c1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'irony-detection-tar2024'...\n",
            "remote: Enumerating objects: 3849, done.\u001b[K\n",
            "remote: Counting objects: 100% (896/896), done.\u001b[K\n",
            "remote: Compressing objects: 100% (383/383), done.\u001b[K\n",
            "remote: Total 3849 (delta 623), reused 770 (delta 506), pack-reused 2953 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3849/3849), 82.18 MiB | 13.30 MiB/s, done.\n",
            "Resolving deltas: 100% (2898/2898), done.\n",
            "Updating files: 100% (1380/1380), done.\n",
            "/content/irony-detection-tar2024/nemojte\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/bpuvaca/irony-detection-tar2024.git\n",
        "%cd irony-detection-tar2024/nemojte/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "import Loader\n",
        "\n",
        "#tweets, labels = Loader.parse_dataset(fp=\"../datasets/crossval/irony.csv\", remove_hashtags=True, balance=False, dataset_type='train')\n",
        "tweets, labels = Loader.parse_dataset(fp=\"../datasets/crossval/sarcasm.csv\", remove_hashtags=True, balance=False, dataset_type='train')\n",
        "#tweets, labels = Loader.parse_dataset(fp=\"../datasets/crossval/semeval_mix.csv\", remove_hashtags=True, balance=False, dataset_type='train')\n",
        "tweets, labels = tweets[:350], labels[:350]"
      ],
      "metadata": {
        "id": "jPFatL8OJ3gT",
        "outputId": "5de75a63-c4d2-4598-862a-c502ab3665f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n",
            "Parsed dataset type train with 1786 tweets, 893 1s and 893 0s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n"
      ],
      "metadata": {
        "id": "bPwZHyI4Vlno"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%capture\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "print(f\"CUDA Major Version: {major_version}\")\n",
        "print(f\"CUDA Minor Version: {minor_version}\")\n",
        "print(\"CUDA version\", torch.version.cuda)\n",
        "print(\"torch version\", torch.__version__)"
      ],
      "metadata": {
        "id": "c4glKyMuHSKY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install accelerate\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "tuOxJlERmYdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(model_name = \"unsloth/llama-3-8b-bnb-4bit\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ],
      "metadata": {
        "id": "KNzPOT-_oNce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sarcasm_prompt = \"\"\"\n",
        "### Instruction:\n",
        "Analyze the following tweet to determine if it contains sarcasm. For this task, we define sarcasm as {}. Respond with a one-word answer: \"Yes\" if the tweet is sarcastic, or \"No\" if it is not.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "sarcasm_definition_cambridge = \"the use of remarks that clearly mean the opposite of what they say, made in order to hurt someone's feelings or to criticize something in a humorous way\"\n",
        "sarcasm_definition_iSarcasm = \"a form of irony that occurs when there is some discrepancy between the literal and intended meanings of an utterance. This discrepancy is used to express dissociation towards a previous proposition, often in the form of contempt or derogation. Tweets that contain sarcasm are those that contradict the state of affairs and are critical towards an addressee.\"\n",
        "\n",
        "irony_prompt = \"\"\"\n",
        "### Instruction:\n",
        "Analyze the following tweet to determine if it contains irony. For this task, we define irony as {}. Respond with a one-word answer: \"Yes\" if the tweet is ironic, or \"No\" if it is not.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "irony_definition_webster = \"the use of words to express something other than and especially the opposite of the literal meaning\"\n",
        "irony_definition_iSarcasm = \"the use of words to express something other than and especially the opposite of the literal meaning. Tweets that contain irony are tweets that contradict the state of affairs but are not obviously critical towards an addressee\""
      ],
      "metadata": {
        "id": "JEVLN2Y4MyBU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_predictions(tweets, prompt, definition):\n",
        "  predictions = []\n",
        "  for tweet in tweets:\n",
        "    inputs = tokenizer([prompt.format(definition, tweet, \"\")], return_tensors = \"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 5)\n",
        "    prediction = tokenizer.batch_decode(outputs)[0]\n",
        "    predictions.append(prediction)\n",
        "    #print(f\"Tweet: {tweet}\\nPrediction: {prediction}\\n\")\n",
        "  return predictions\n"
      ],
      "metadata": {
        "id": "f14XO82bNjEX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = generate_predictions(tweets, irony_prompt, irony_definition_iSarcasm)"
      ],
      "metadata": {
        "id": "RryWdXA_AnoR"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_predictions = []\n",
        "for prediction in predictions:\n",
        "  try:\n",
        "    response = prediction.split(\"### Response:\")[1].strip().split(\"\\n\")[0].strip()\n",
        "    if \"no\" in response.lower():\n",
        "      response = 0\n",
        "    else:\n",
        "      response = 1\n",
        "    new_predictions.append(response)\n",
        "  except IndexError:\n",
        "    new_predictions.append(None)\n",
        "\n",
        "predictions = new_predictions"
      ],
      "metadata": {
        "id": "-FzJ5QSfAsHM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet, label, prediction in zip(tweets, labels, predictions):\n",
        "  print(f\"Tweet: {tweet}\\nLabel: {label}\\nPrediction: {prediction}\\n\")"
      ],
      "metadata": {
        "id": "m6JAn2LeOIkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def save_to_csv(tweets, labels, predictions, filename):\n",
        "  with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Tweet', 'Label', 'Prediction'])\n",
        "    for tweet, label, prediction in zip(tweets, labels, predictions):\n",
        "      writer.writerow([tweet, label, prediction])\n",
        "\n",
        "file_name = \"LLM_sarcasm_results_irony.csv\"\n",
        "save_to_csv(tweets, labels, predictions, file_name)\n"
      ],
      "metadata": {
        "id": "VqJfAWR9OWOg"
      },
      "execution_count": 38,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}