{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "JY2-kNQPGbxp"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/bpuvaca/irony-detection-tar2024.git\n",
        "%cd irony-detection-tar2024/nemojte/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "id": "C7MqE8irP99u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import Loader\n",
        "\n",
        "tweets, labels = Loader.parse_dataset(fp=\"../datasets/iSarcasm/sarcasm_test.csv\", remove_hashtags=True, balance=False, dataset_type='train')"
      ],
      "metadata": {
        "id": "jPFatL8OJ3gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "# Must install separately since Colab has torch 2.2.1, which breaks packages\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "pass"
      ],
      "metadata": {
        "id": "c4glKyMuHSKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(model_name = \"unsloth/llama-3-8b-bnb-4bit\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ],
      "metadata": {
        "id": "KNzPOT-_oNce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sarcasm_prompt = \"\"\"\n",
        "### Instruction:\n",
        "Analyze the following tweet to determine if it is sarcastic. For this task, we define sarcasm as {}. Respond with a one-word answer: \"Yes\" if the tweet is sarcastic, or \"No\" if it is not.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "sarcasm_definition_cambridge = \"the use of remarks that clearly mean the opposite of what they say, made in order to hurt someone's feelings or to criticize something in a humorous way\""
      ],
      "metadata": {
        "id": "JEVLN2Y4MyBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_predictions(tweets, definition):\n",
        "  predictions = []\n",
        "  for tweet in tweets:\n",
        "    inputs = tokenizer([sarcasm_prompt.format(definition, tweet, \"\")], return_tensors = \"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 5)\n",
        "    prediction = tokenizer.batch_decode(outputs)[0]\n",
        "    predictions.append(prediction)\n",
        "  return predictions\n",
        ""
      ],
      "metadata": {
        "id": "f14XO82bNjEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = generate_predictions(tweets, sarcasm_definition_cambridge)\n",
        "\n",
        "for tweet, label, prediction in zip(tweets, labels, predictions):\n",
        "  print(f\"Tweet: {tweet}\\nLabel: {label}\\nPrediction: {prediction}\\n\")"
      ],
      "metadata": {
        "id": "m6JAn2LeOIkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def save_to_csv(tweets, labels, predictions, filename=\"isarcasm_predictions.csv\"):\n",
        "  with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Tweet', 'Label', 'Prediction'])\n",
        "    for tweet, label, prediction in zip(tweets, labels, predictions):\n",
        "      writer.writerow([tweet, label, prediction])\n",
        "\n",
        "save_to_csv(tweets, labels, predictions)"
      ],
      "metadata": {
        "id": "VqJfAWR9OWOg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}