{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "JY2-kNQPGbxp",
        "outputId": "faca45f8-99c1-4ead-d688-c9f460c1403f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'irony-detection-tar2024'...\n",
            "remote: Enumerating objects: 1032, done.\u001b[K\n",
            "remote: Counting objects: 100% (284/284), done.\u001b[K\n",
            "remote: Compressing objects: 100% (192/192), done.\u001b[K\n",
            "remote: Total 1032 (delta 176), reused 177 (delta 92), pack-reused 748 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1032/1032), 8.36 MiB | 14.96 MiB/s, done.\n",
            "Resolving deltas: 100% (652/652), done.\n",
            "/content/irony-detection-tar2024/nemojte\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/bpuvaca/irony-detection-tar2024.git\n",
        "%cd irony-detection-tar2024/nemojte/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "import Loader\n",
        "\n",
        "#tweets, labels = Loader.parse_dataset(fp=\"../datasets/iSarcasm/sarcasm_test.csv\", remove_hashtags=True, balance=False, dataset_type='train')\n",
        "tweets, labels = Loader.parse_dataset(fp=\"../datasets/SemEval2018/polarity_test.csv\", remove_hashtags=True, balance=False, dataset_type='train')"
      ],
      "metadata": {
        "id": "jPFatL8OJ3gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n"
      ],
      "metadata": {
        "id": "bPwZHyI4Vlno"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%capture\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "print(f\"CUDA Major Version: {major_version}\")\n",
        "print(f\"CUDA Minor Version: {minor_version}\")\n",
        "print(\"CUDA version\", torch.version.cuda)\n",
        "print(\"torch version\", torch.__version__)"
      ],
      "metadata": {
        "id": "c4glKyMuHSKY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install accelerate\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "tuOxJlERmYdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(model_name = \"unsloth/llama-3-8b-bnb-4bit\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ],
      "metadata": {
        "id": "KNzPOT-_oNce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sarcasm_prompt = \"\"\"\n",
        "### Instruction:\n",
        "Analyze the following tweet to determine if it is sarcastic. For this task, we define sarcasm as {}. Respond with a one-word answer: \"Yes\" if the tweet is sarcastic, or \"No\" if it is not.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "sarcasm_definition_cambridge = \"the use of remarks that clearly mean the opposite of what they say, made in order to hurt someone's feelings or to criticize something in a humorous way\"\n",
        "\n",
        "irony_prompt = \"\"\"\n",
        "### Instruction:\n",
        "Analyze the following tweet to determine if it contains irony. For this task, we define irony as {}. Respond with a one-word answer: \"Yes\" if the tweet is ironic, or \"No\" if it is not.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "irony_definition_webster = \"the use of words to express something other than and especially the opposite of the literal meaning\""
      ],
      "metadata": {
        "id": "JEVLN2Y4MyBU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_predictions(tweets, prompt, definition):\n",
        "  predictions = []\n",
        "  for tweet in tweets:\n",
        "    inputs = tokenizer([prompt.format(definition, tweet, \"\")], return_tensors = \"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 5)\n",
        "    prediction = tokenizer.batch_decode(outputs)[0]\n",
        "    predictions.append(prediction)\n",
        "    #print(f\"Tweet: {tweet}\\nPrediction: {prediction}\\n\")\n",
        "  return predictions\n"
      ],
      "metadata": {
        "id": "f14XO82bNjEX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = generate_predictions(tweets, irony_prompt, irony_definition_webster)"
      ],
      "metadata": {
        "id": "RryWdXA_AnoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_predictions = []\n",
        "for prediction in predictions:\n",
        "  try:\n",
        "    response = prediction.split(\"### Response:\")[1].strip().split(\"\\n\")[0].strip()\n",
        "    if \"no\" in response.lower():\n",
        "      response = 0\n",
        "    else:\n",
        "      response = 1\n",
        "    new_predictions.append(response)\n",
        "  except IndexError:\n",
        "    new_predictions.append(None)\n",
        "\n",
        "predictions = new_predictions"
      ],
      "metadata": {
        "id": "-FzJ5QSfAsHM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet, label, prediction in zip(tweets, labels, predictions):\n",
        "  print(f\"Tweet: {tweet}\\nLabel: {label}\\nPrediction: {prediction}\\n\")"
      ],
      "metadata": {
        "id": "m6JAn2LeOIkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def save_to_csv(tweets, labels, predictions, filename):\n",
        "  with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Tweet', 'Label', 'Prediction'])\n",
        "    for tweet, label, prediction in zip(tweets, labels, predictions):\n",
        "      writer.writerow([tweet, label, prediction])\n",
        "\n",
        "file_name = \"irony_llm_predictions.csv\"\n",
        "save_to_csv(tweets, labels, predictions, file_name)\n"
      ],
      "metadata": {
        "id": "VqJfAWR9OWOg"
      },
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}